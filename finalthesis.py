# -*- coding: utf-8 -*-
"""FINALTHESIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qMHz7QdcxidGHOS3aHTXolnGpusjApjw
"""

# pip install torch-geometric

# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

import pandas as pd
import numpy as np
import networkx as nx
import os
from math import sqrt
import torch
from torch import nn
import torch.nn.functional as F
from torch.nn import (Embedding, Sequential, Linear, Sigmoid, ReLU, Dropout, LayerNorm, Softplus)
from torch.nn.functional import one_hot, softmax
from torch_geometric.nn import GAT
from torch_geometric.utils import dense_to_sparse, to_edge_index
from collections import defaultdict
from torch_geometric.utils import add_self_loops


folder_path = 'input/raw'

question_df = pd.read_csv(f"{folder_path}/Questions.csv")
option_df = pd.read_csv(f"{folder_path}/Question_Choices.csv")
concept_df = pd.read_csv(f"{folder_path}/KCs.csv")
relation_df = pd.read_csv(f"{folder_path}/KC_Relationships.csv")
question_concept_df = pd.read_csv(f"{folder_path}/Question_KC_Relationships.csv")
interaction_df = pd.read_csv(f"{folder_path}/Transaction.csv")
specialization_df = pd.read_csv(f"{folder_path}/Specialization.csv")
student_specialization_df = pd.read_csv(f"{folder_path}/Student_Specialization.csv")

# ----------------------- TIỀN XỬ LÝ -----------------------
def clean_data():
    global question_df, option_df, concept_df, relation_df, question_concept_df, interaction_df

    question_df.drop_duplicates(subset=["id"], inplace=True)
    question_concept_df.drop_duplicates(subset=["question_id", "knowledgecomponent_id"], inplace=True)
    concept_df.drop_duplicates(subset=["id"], inplace=True)
    option_df.drop_duplicates(subset=["id", "question_id"], inplace=True)
    interaction_df.drop_duplicates(subset=["student_id", "question_id", "start_time"], inplace=True)
    relation_df.drop_duplicates(subset=["from_knowledgecomponent_id", "to_knowledgecomponent_id"], inplace=True)

    concept_df["name"] = concept_df["name"].fillna("Unknown Concept")
    question_df["question_text"] = question_df["question_text"].fillna("")
    option_df["choice_text"] = option_df["choice_text"].fillna("")

    # Xử lý thời gian
    if not pd.api.types.is_datetime64_any_dtype(interaction_df["start_time"]):
        interaction_df["start_time"] = pd.to_datetime(interaction_df["start_time"], errors="coerce")

    interaction_df.dropna(subset=["start_time"], inplace=True)
    if pd.api.types.is_datetime64tz_dtype(interaction_df["start_time"]):
        interaction_df["start_time"] = interaction_df["start_time"].dt.tz_localize(None)

    before = len(interaction_df)
    interaction_df.dropna(subset=["student_id", "question_id", "answer_state"], inplace=True)
    after = len(interaction_df)

    print(f"[clean_data] Số dòng interaction còn lại: {after} (loại {before - after})")

# ----------------------- ÁNH XẠ ID -----------------------
def map_ids():
    global user2idx, question2idx, kc2idx, user_list, question_list, kc_list

    user_list = sorted(interaction_df["student_id"].unique())
    question_list = sorted(question_df["id"].unique())
    kc_list = sorted(concept_df["id"].unique())

    user2idx = {u: i for i, u in enumerate(user_list)}
    question2idx = {q: i for i, q in enumerate(question_list)}
    kc2idx = {k: i for i, k in enumerate(kc_list)}

    print(f"[map_ids] Số user: {len(user2idx)}, question: {len(question2idx)}, KC: {len(kc2idx)}")

# ----------------------- TẠO EVENTS -----------------------
def process_events():
    global interaction_df

    interaction_df["user_idx"] = interaction_df["student_id"].map(user2idx)
    interaction_df["question_idx"] = interaction_df["question_id"].map(question2idx)
    interaction_df["timestamp"] = interaction_df["start_time"].apply(lambda x: int(x.timestamp()))
    interaction_df["is_correct"] = interaction_df["answer_state"].astype(int)

# ----------------------- MAP CÂU TRẢ LỜI -----------------------
def map_answers():
    global interaction_df

    option_df_sorted = option_df.sort_values(["question_id", "id"])
    option_map = option_df_sorted.groupby("question_id")["id"].apply(list).to_dict()

    def convert_answer(qid, aid):
        if qid not in option_map or pd.isna(aid):
            return None
        try:
            return option_map[qid].index(aid)
        except ValueError:
            return None

    interaction_df["answer_idx"] = interaction_df.apply(
        lambda x: convert_answer(x["question_id"], x["answer_choice_id"]), axis=1
    )

# ----------------------- MAP CONCEPT -----------------------
def map_concepts():
    global interaction_df

    qkc_map = question_concept_df.groupby("question_id")["knowledgecomponent_id"].apply(list).to_dict()

    interaction_df["concept_idxs"] = interaction_df["question_id"].apply(
        lambda qid: [kc2idx[k] for k in qkc_map.get(qid, []) if k in kc2idx]
    )

    interaction_df = interaction_df[interaction_df["concept_idxs"].map(len) > 0]

# ----------------------- SORT EVENTS -----------------------
def sort_events():
    global interaction_df
    interaction_df.sort_values(["user_idx", "timestamp"], inplace=True)
    interaction_df.reset_index(drop=True, inplace=True)

# ----------------------- TẠO GRAPH KIẾN THỨC -----------------------
def create_concept_graph(relation_df, kc2idx, num_c):
    relation_df["src_idx"] = relation_df["from_knowledgecomponent_id"].map(kc2idx)
    relation_df["tar_idx"] = relation_df["to_knowledgecomponent_id"].map(kc2idx)
    rel_df = relation_df.dropna()

    edges = list(zip(rel_df["src_idx"], rel_df["tar_idx"]))
    filtered_edges = set()

    for s, t in edges:
        if s == t:
            continue
        if (t, s) in filtered_edges:
            continue
        filtered_edges.add((s, t))
    directed = list(filtered_edges)

    knowledge_mask = torch.zeros((num_c, num_c))
    for s, t in directed:
        if 0 <= s < num_c and 0 <= t < num_c:
            knowledge_mask[s, t] = 1
    concept_graph = {"directed": directed}
    print(f"[create_concept_graph] Directed: {len(directed)}")
    return concept_graph, knowledge_mask

# ----------------------- TẠO SNAPSHOT -----------------------
def create_snapshots():
    snapshots = {}
    for uid, group in interaction_df.groupby("user_idx"):
        times = group["timestamp"].tolist()
        concepts = group["concept_idxs"].tolist()

        cum_concepts = set()
        snap = []
        for t, cset in zip(times, concepts):
            cum_concepts.update(cset)
            snap.append((t, list(cum_concepts)))
        snapshots[uid] = snap

    print(f"[create_snapshots] Số người dùng có snapshot: {len(snapshots)}")
    return snapshots

def create_knowledge_graph_dcrkt(snapshot_mv, concept_df, kc_list, snapshot_graph=None, threshold_sim=0.2, knowledge_mask=None, kc_mapping=None ):
    """
    Tạo bản đồ kiến thức từ snapshot Mv của học sinh tại 1 thời điểm cụ thể.

    Params:
    - snapshot_mv: Tensor [num_concepts, dim] – snapshot Mv từ model.get_snapshot(student_id, step)
    - concept_df: DataFrame chứa thông tin khái niệm (id, name)
    - kc_list: List[int] – danh sách ID khái niệm gốc tương ứng với chỉ số trong Mv
    - snapshot_graph: (edge_index, edge_weight) tuple (tùy chọn) từ DCRKT.snapshots_graph
    - threshold_sim: ngưỡng similarity để vẽ cạnh nếu không dùng snapshot_graph

    Returns:
    - G: networkx.DiGraph với node = concept, edge = lan truyền hiểu biết
    """
    G = nx.DiGraph()

    # Chuẩn hóa tên khái niệm
    idx_to_name = {}
    for idx, cid in enumerate(kc_list):
        row = concept_df[concept_df["id"] == cid]
        name = row["name"].values[0] if not row.empty else f"Concept {cid}"
        idx_to_name[idx] = name

    # Tính norm từng vector Mv → biểu thị độ hiểu biết
    mastery_norms = [vec.norm().item() for vec in snapshot_mv]

    # Thêm node
    for idx, norm in enumerate(mastery_norms):
        cid = kc_list[idx]
        cname = idx_to_name.get(idx, f"Concept {cid}")
        color = "red" if norm < 1.0 else "orange" if norm < 2.0 else "green"
        G.add_node(idx, concept_id=cid, name=cname, mastery=norm, color=color)
    # Thêm cạnh dựa trên snapshot_graph nếu có
    if snapshot_graph:
        edge_index, edge_weight = snapshot_graph
        for i in range(edge_index.shape[1]):
            src = edge_index[0, i].item()
            tgt = edge_index[1, i].item()
            if src == tgt:
                continue
            global_src = kc_mapping[src] if kc_mapping else src
            global_tgt = kc_mapping[tgt] if kc_mapping else tgt
            if knowledge_mask is not None and knowledge_mask[src, tgt].item() == 0:
                continue
            weight = edge_weight[i].item()
            G.add_edge(src, tgt, weight=weight, relation_type="propagated")
    else:
        # Tự tính cạnh dựa trên độ tương đồng
        normed = snapshot_mv / snapshot_mv.norm(dim=1, keepdim=True).clamp(min=1e-6)
        sim_matrix = torch.matmul(normed, normed.T).cpu().numpy()
        for i in range(len(kc_list)):
            for j in range(i + 1, len(kc_list)):
                if sim_matrix[i][j] >= threshold_sim:
                      global_i = kc_mapping[i] if kc_mapping else i
                      global_j = kc_mapping[j] if kc_mapping else j
                      if knowledge_mask is None or knowledge_mask[i, j] == 1:
                          G.add_edge(i, j, weight=sim_matrix[i][j], relation_type="similarity")
                      if knowledge_mask is None or knowledge_mask[j, i] == 1:
                          G.add_edge(j, i, weight=sim_matrix[i][j], relation_type="similarity")
                    # G.add_edge(i, j, weight=sim_matrix[i][j], relation_type="similarity")
                    # G.add_edge(j, i, weight=sim_matrix[i][j], relation_type="similarity")

    return G

def visualize_student_knowledge_graph(student_id, step, model, show_prerequisite=False):
    mv = model.get_snapshot(student_id, step)
    snapshot_graph = model.snapshots_graph[student_id][step]
    G = create_knowledge_graph_dcrkt(mv, concept_df, kc_list, snapshot_graph=snapshot_graph, knowledge_mask=knowledge_mask)

    if show_prerequisite:
        _, prereq_mask = create_concept_graph(relation_df, kc2idx, num_c)
        for i in range(num_c):
            for j in range(num_c):
                if prereq_mask[i, j] == 1 and i in G.nodes and j in G.nodes:
                    G.add_edge(i, j, weight=1.0, relation_type="prerequisite")
    return G


# ----------------------- TẠO CONCEPT MEMORY -----------------------
def create_concept_memory():
    memory = {cid: {"Mk": None, "Mv": None} for cid in range(len(kc_list))}
    print("[create_concept_memory] Khởi tạo concept memory hoàn tất.")
    return memory

# ----------------------- HÀM CHÍNH -----------------------
def run_all_steps():
    clean_data()
    map_ids()
    num_c = len(kc2idx)
    relation_dict = defaultdict(set)
    for _, row in relation_df.iterrows():
        src = kc2idx.get(row["from_knowledgecomponent_id"])
        tgt = kc2idx.get(row["to_knowledgecomponent_id"])
        if src is not None and tgt is not None:
            relation_dict[src].add(tgt)
            relation_dict[tgt].add(src)

    process_events()
    map_answers()
    map_concepts()
    sort_events()
    concept_graph, knowledge_mask = create_concept_graph(relation_df, kc2idx, num_c)
    snapshots = create_snapshots()
    concept_memory = create_concept_memory()

    # Đường dẫn lưu file processed
    output_path = "data/raw/processed_data.csv"

    # Tạo thư mục nếu chưa tồn tại
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # Lưu file CSV đã xử lý
    interaction_df.to_csv(output_path, index=False)
    print(f"[run_all_steps] Dữ liệu đã được lưu tại: {output_path}")

    # return snapshots, concept_graph, concept_memory, interaction_df
    return snapshots, concept_graph, concept_memory, interaction_df, len(question2idx), len(kc2idx), interaction_df["answer_idx"].nunique(), relation_dict, knowledge_mask

# Gọi hàm chính nếu chạy trực tiếp
if __name__ == "__main__":
    snapshots, concept_graph, concept_memory, interaction_df, num_q, num_c, num_o, relation_dict, knowledge_mask  = run_all_steps()

# MODULE: Multi-head Attention
class MultiHeadAttention(nn.Module):
    def __init__(self, dim, num_heads, dropout):
        super().__init__()
        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)
        self.norm = LayerNorm(dim)
        self.drop = Dropout(dropout)

    def forward(self, query, key, value):
        out, _ = self.attn(query, key, value)
        return self.norm(query + self.drop(out))

# MODULE 1: Disentangled Response Encoder
class DisentangledResponseEncoder(nn.Module):
    def __init__(self, dim_q, dropout):
        super().__init__()
        dim_h = dim_q // 2
        self.enc_correct = Sequential(Linear(dim_q, dim_h), ReLU(), Dropout(dropout), Linear(dim_h, dim_q))
        self.enc_wrong = Sequential(Linear(dim_q, dim_h), ReLU(), Dropout(dropout), Linear(dim_h, dim_q))
        self.enc_unchosen = Sequential(Linear(dim_q, dim_h), ReLU(), Dropout(dropout), Linear(dim_h, dim_q))
        self.attn_response = MultiHeadAttention(dim_q, 2, dropout)

    def forward(self, ot, ut, score):
        correct_mask = score == 1
        wrong_mask = score == 0
        ot_prime = torch.zeros_like(ot)
        ot_prime[correct_mask] = self.enc_correct(ot[correct_mask])
        ot_prime[wrong_mask] = self.enc_wrong(ot[wrong_mask])
        ut_prime = self.enc_unchosen(ut)
        d_t = ot_prime - ut_prime
        d_t_hat = self.attn_response(d_t, d_t, d_t)
        return d_t_hat

# MODULE 2: Knowledge Retriever
class KnowledgeRetriever(nn.Module):
    def __init__(self, dim_q, num_heads, dropout):
        super().__init__()
        self.attn_question = MultiHeadAttention(dim_q, num_heads, dropout)
        self.attn_state = MultiHeadAttention(dim_q, num_heads, dropout)

    def forward(self, qt, d_t, concept_ids):# thêm concept_ids
        qt_hat = self.attn_question(qt, qt, qt)
        h_t_list = []
        for _ in concept_ids:
            h_t_cid = self.attn_state(qt_hat, qt_hat, d_t)
            h_t_list.append(h_t_cid.squeeze(0))
        h_t_final = torch.stack(h_t_list, dim=0)  # [len(concept_ids), D]
        return qt_hat, h_t_final
# MODULE 3: Per-Concept Memory with Forget Gate + Time Decay
class MemoryUpdater(nn.Module):
    def __init__(self, dim_g, decay_scale=0.5):
        super().__init__()
        self.decay_scale = decay_scale
        self.forget_gate = Linear(dim_g + 1, 1)
    def forward(self, memory_value, delta_t, response_update):
        if delta_t.dim() == 1:
            delta_t = delta_t.unsqueeze(-1)  # [num_c, 1]
        time_feat = torch.log1p(delta_t.float()) * self.decay_scale
        gate_input = torch.cat([memory_value, time_feat], dim=-1)
        gamma = torch.sigmoid(self.forget_gate(gate_input))
        updated_memory = gamma * memory_value + (1 - gamma) * response_update
        return updated_memory

# MODULE 4: Dynamic Latent Concept Graph Builder
class DynamicConceptGraphBuilder(nn.Module):
    def __init__(self, num_c, top_k, knowledge_mask):
        super().__init__()
        self.num_c = num_c
        self.top_k = top_k
        self.knowledge_mask = knowledge_mask

    def forward(self, memory_value, attn_weights=None, knowledge_mask=None):
        num_nodes = memory_value.size(0)
        normed = memory_value / memory_value.norm(dim=1, keepdim=True).clamp(min=1e-6)
        sim_matrix = torch.matmul(normed, normed.T)
        if attn_weights is not None:
            attn_outer = torch.ger(attn_weights, attn_weights)  # outer product
            sim_matrix = sim_matrix * (0.5 + 0.5 * attn_outer)

        sim_matrix = sim_matrix * (sim_matrix > 0.05).float()
        # sim_matrix.fill_diagonal_(1.0)
        edge_index, edge_weight = dense_to_sparse(sim_matrix)
        edge_index, edge_weight = add_self_loops(edge_index, edge_weight, num_nodes=num_nodes)
        return edge_index, edge_weight

# MODULE 5: Prediction
class AttentionBasedPredictor(nn.Module):
    def __init__(self, dim_q, dim_g):
        super().__init__()
        self.query_proj = Linear(dim_q, dim_g)
        self.sigmoid = Sigmoid()

    def forward(self, qt_hat, memory_key, memory_value):
        pred_query = self.query_proj(qt_hat)
        sim = torch.matmul(pred_query, memory_key.T)
        top_k = min(10, sim.size(-1))
        top_k_val, top_k_idx = torch.topk(sim, top_k)
        attn_mask = torch.full_like(sim, float('-inf'))
        attn_mask[0, top_k_idx[0]] = top_k_val[0]
        attn = torch.softmax(attn_mask, dim=-1)
        mastery = torch.matmul(attn, memory_value)
        logits = torch.sum(pred_query * mastery, dim=-1)
        return self.sigmoid(logits)

class DCRKT(nn.Module):
    def __init__(self, num_c, num_q, num_o, dim_q, dim_g, num_heads, top_k, dropout, knowledge_mask=None):
        super().__init__()
        self.num_c = num_c
        self.num_q = num_q
        self.num_o = num_o
        self.dim_g = dim_g

        self.question_emb = Embedding(num_q + 1, dim_q)
        self.response_emb = Embedding(num_q * num_o + 2, dim_q, padding_idx=-1)

        self.encoder = DisentangledResponseEncoder(dim_q, dropout)
        self.retriever = KnowledgeRetriever(dim_q, num_heads, dropout)
        self.memory_updater = MemoryUpdater(dim_g, decay_scale=0.5)
        self.graph_builder = DynamicConceptGraphBuilder(num_c, top_k, knowledge_mask)
        self.predictor = AttentionBasedPredictor(dim_q, dim_g)
        self.knowledge_mask = knowledge_mask
        self.memory_key = nn.Parameter(torch.randn(num_c, dim_g))

        # === NEW: Per-student memory ===
        self.student_memory = {}
        self.last_update_time = {}
        self.snapshots = {}  # dict[student_id] → list of Mv snapshot over time
        self.snapshots_graph = {}
        self.max_snapshot_edges = 7
        self.gat = GAT(in_channels=dim_g, hidden_channels=dim_g // 2,
                       out_channels=dim_g, num_layers=2, dropout=dropout)

    def _get_student_memory(self, student_id):
        if student_id not in self.student_memory:
            self.student_memory[student_id] = torch.zeros(self.num_c, self.dim_g)
        return self.student_memory[student_id]
    
    def _get_last_update_time(self, student_id):
        if student_id not in self.last_update_time:
            self.last_update_time[student_id] = torch.zeros(self.num_c)
        return self.last_update_time[student_id]

    def reset_memory(self, student_id):
        self.student_memory[student_id] = torch.zeros(self.num_c, self.dim_g)
        self.last_update_time[student_id] = torch.zeros(self.num_c)
        self.snapshots[student_id] = []
        self.snapshots_graph[student_id] = []

    def forward_single_step(self, student_id, q_idx, o_idx, u_idx, score, timestamp, concept_ids):
        concept_ids = [cid for cid in concept_ids if 0 <= cid < self.num_c]
        if len(concept_ids) == 0:
            return torch.tensor(0.5)
        device = self.memory_key.device
        q_idx = torch.clamp(q_idx, 0, self.num_q - 1)
        o_idx = torch.clamp(o_idx, 0, self.num_o - 1)
        u_idx = torch.clamp(u_idx, 0, self.num_o - 1)
        response_idx = q_idx * self.num_o + o_idx
        unchosen_idx = q_idx * self.num_o + u_idx
        max_idx = self.response_emb.num_embeddings - 1
        if response_idx > max_idx or unchosen_idx > max_idx:
            raise ValueError(f"[Embedding Overflow] response_idx={response_idx}, unchosen_idx={unchosen_idx}, max={max_idx}")
        qt = self.question_emb(q_idx)
        ot = self.response_emb(q_idx * self.num_o + o_idx)
        ut = self.response_emb(q_idx * self.num_o + u_idx)

        d_t = self.encoder(ot.unsqueeze(0), ut.unsqueeze(0), score.unsqueeze(0))
        qt_hat, h_t_all = self.retriever(qt.unsqueeze(0), d_t, concept_ids)

        mv = self._get_student_memory(student_id).to(device)
        mk = self.memory_key.to(device)
        last_time = self._get_last_update_time(student_id).to(device)

        # === UPDATE MEMORY CHỈ TRÊN CONCEPT LIÊN QUAN ===
        current_time = timestamp.item()  # thời gian hiện tại
        delta_t = torch.zeros_like(last_time)
        for cid in concept_ids:
            delta_t[cid] = current_time - last_time[cid]
            last_time[cid] = current_time

        all_seen = set([i for i in range(self.num_c) if mv[i].norm() > 0])
        all_seen.update(concept_ids)
        if self.knowledge_mask is not None:
            for cid in concept_ids:
                related = (self.knowledge_mask[cid] > 0).nonzero(as_tuple=True)[0]
                all_seen.update(related.tolist())
        RECENT_SNAPSHOT = 3
        EDGE_WEIGHT_THRESHOLD = 0.05  # tùy chỉnh theo kinh nghiệm

        if student_id in self.snapshots_graph and self.snapshots_graph[student_id]:
            recent_snapshots = self.snapshots_graph[student_id][-RECENT_SNAPSHOT:]  # lấy snapshot gần nhất
            for prev_edges, prev_weights in recent_snapshots:
                for cid in concept_ids:
                    cid_tensor = torch.tensor(cid, device=prev_edges.device)

                    # Lấy các cạnh có trọng số đủ lớn
                    mask_from = (prev_edges[0] == cid_tensor) & (prev_weights > EDGE_WEIGHT_THRESHOLD)
                    connected_from = prev_edges[1][mask_from].tolist()

                    mask_to = (prev_edges[1] == cid_tensor) & (prev_weights > EDGE_WEIGHT_THRESHOLD)
                    connected_to = prev_edges[0][mask_to].tolist()
                    all_seen.update(connected_from)
                    all_seen.update(connected_to)

        seen_list = [i for i in all_seen if 0 <= i < self.num_c]
        if len(seen_list) == 0:
            return torch.tensor(0.5, device=device)

        seen_tensor = torch.tensor(seen_list, dtype=torch.long, device=device)
        mv_filtered = torch.index_select(mv, 0, seen_tensor)
        if mv_filtered.dim() == 1:
            mv_filtered = mv_filtered.unsqueeze(0)

        masked_kmask = self.knowledge_mask[seen_tensor][:, seen_tensor] if self.knowledge_mask is not None else None

        pred_query = self.predictor.query_proj(qt_hat)
        pred_query = F.normalize(pred_query, dim=-1)
        memory_key = F.normalize(mk, dim=-1)
        sim = torch.matmul(pred_query, memory_key.T)
        num_concepts = sim.size(-1)
        top_k = min(10, num_concepts)
        if top_k == 0:
            return torch.tensor(0.5, device=self.memory_key.device)
        top_k_val, top_k_idx = torch.topk(sim, k=top_k, dim=-1)

        for cid in concept_ids:
            if cid not in top_k_idx[0]:
                top_k_idx[0][-1] = cid
                top_k_val[0][-1] = sim[0, cid]
        attn_masked = torch.full_like(sim, float('-inf'))
        attn_masked[0, top_k_idx[0]] = top_k_val[0]
        attn_weights = F.softmax(attn_masked, dim=-1).squeeze(0)
        attn_weights_subset = attn_weights[seen_tensor]
        # Boost: nếu α_k > 0.1 thì scale lên để tăng ảnh hưởng
        attn_weights_boosted = attn_weights_subset.clone()
        attn_weights_boosted[attn_weights_subset > 0.1] *= 1.5
        attn_weights_boosted = attn_weights_boosted / attn_weights_boosted.sum()
        if masked_kmask is not None:
          boost = masked_kmask.sum(dim=1).float()
          boost = boost / (boost.sum() + 1e-6)
          attn_weights_boosted += 0.3 * boost
          attn_weights_boosted = attn_weights_boosted / attn_weights_boosted.sum()
        edge_index, edge_weight = self.graph_builder(mv_filtered, attn_weights=attn_weights_boosted ,knowledge_mask=masked_kmask )
        # print(f"[GraphBuilder] seen={len(seen_list)}, kmask_sum={masked_kmask.sum().item() if masked_kmask is not None else 'None'}, edge_count={edge_index.shape[1]}")
        # print(f"[α_k] min={attn_weights_subset.min().item():.4f}, max={attn_weights_subset.max().item():.4f}")
        # print("=" * 40)
        # print(f"   [Step Debug] student_id={student_id}, q_idx={q_idx.item()}, score={score.item()}")
        # print(f"   Concept IDs mapped: {concept_ids}")
        # print(f"   Attention Weights (α_k): {[round(attn_weights[cid].item(), 4) for cid in concept_ids]}")
        # print(f"   Max α_k: {attn_weights.max().item():.4f} | Sim range: {sim.min().item():.4f} → {sim.max().item():.4f}")
        # print(f"response_idx = {response_idx}, emb norm = {ot.norm().item():.4f}")
        idx_map = {i: cid for i, cid in enumerate(seen_list)}
        edge_index = edge_index.cpu().numpy()
        edge_index_mapped = torch.tensor([[idx_map[int(i)] for i in edge_index[0]], [idx_map[int(i)] for i in edge_index[1]]], device=mv.device, dtype=torch.long)
        if self.knowledge_mask is not None:
            extra_edges = []
            extra_weights = []
            for i in seen_list:
                for j in seen_list:
                    if self.knowledge_mask[i, j] == 1:
                        exists = ((edge_index_mapped[0] == i) & (edge_index_mapped[1] == j)).any()
                        if not exists:
                            extra_edges.append([i, j])
                            extra_weights.append(0.05)  # trọng số nhỏ để giữ ảnh hưởng thấp

            if extra_edges:
                extra_edges = torch.tensor(extra_edges, device=mv.device, dtype=torch.long).T  # [2, num_extra]
                edge_index_mapped = torch.cat([edge_index_mapped, extra_edges], dim=1)
                edge_weight = torch.cat([edge_weight, torch.tensor(extra_weights, device=mv.device)])
        mv_propagated = self.gat(mv, edge_index_mapped, edge_weight.to(device))
        if student_id not in self.snapshots_graph:
            self.snapshots_graph[student_id] = []
        self.snapshots_graph[student_id].append((edge_index_mapped.detach().cpu(), edge_weight.detach().cpu()))

        response_update = torch.zeros_like(mv)
        mv_updated = mv.detach().clone()
        for i, cid in enumerate(concept_ids):
            h_update = h_t_all[i]
            alpha = attn_weights[cid].item()
            response_update[cid] = alpha * h_update if alpha > 1e-4 else h_update
            updated = self.memory_updater(
                mv_propagated[cid].unsqueeze(0),
                delta_t[cid].unsqueeze(0),
                response_update[cid].unsqueeze(0)
            ).detach()
            mv_updated = mv_updated.index_copy(0, torch.tensor([cid], device=mv.device), updated)
            # print(f"⮕ [Trực tiếp] Mv[{cid}] ‖new‖ = {mv_updated[cid].norm().item():.6f}")

        # Cập nhật
        self.student_memory[student_id] = mv_updated.detach()
        self.last_update_time[student_id] = last_time
        if student_id not in self.snapshots:
            self.snapshots[student_id] = []
        self.snapshots[student_id].append(mv_updated.detach().clone())

        # === Dự đoán ===
        pred = self.predictor(qt_hat, mk, mv_updated)
        return pred.squeeze()

    def get_snapshot(self, student_id, step=-1):
        if student_id not in self.snapshots or not self.snapshots[student_id]:
            return None
        return self.snapshots[student_id][step]

"""#test student"""

# snapshots, concept_graph, concept_memory, interaction_df, num_q, num_c, num_o, relation_dict, knowledge_mask = run_all_steps()

# # # Gán lại các biến cần thiết sau tiền xử lý
# question_kc_df = question_concept_df.copy()
# kc_df = concept_df.copy()
# student_df = interaction_df[interaction_df["user_idx"] == user2idx[39]].sort_values("timestamp")
# history_df = student_df[student_df["question_idx"] != question2idx[11]]

# student_df = interaction_df[interaction_df["user_idx"] == user2idx[39]].sort_values("timestamp")
# num_attempts = len(student_df)
# print(f"Học sinh 39 đã học {num_attempts} lần.")

# # # Gán lại các biến cần thiết sau tiền xử lý
# question_kc_df = question_concept_df.copy()
# kc_df = concept_df.copy()
# student_df = interaction_df[interaction_df["user_idx"] == user2idx[39]].sort_values("timestamp")
# history_df = student_df[student_df["question_idx"] != question2idx[11]]

# concept_graph, knowledge_mask = create_concept_graph(relation_df, kc2idx, num_c)

# model = DCRKT(num_c=num_c,num_q=num_q,num_o=num_o,dim_q=32,dim_g=32,num_heads=2,top_k=3,dropout=0.2, knowledge_mask=knowledge_mask)
# model.reset_memory(39)

# # 1. Lấy lịch sử học sinh 39
# history_df = interaction_df[interaction_df["user_idx"] == user2idx[39]]

# # 2. Duyệt qua từng tương tác
# for _, row in history_df.iterrows():
#     qid = int(row["question_id"])
#     oid = int(row["answer_choice_id"])
#     uid = max(0, oid - 1)
#     score = int(row["answer_state"])
#     timestamp = pd.to_datetime(row["start_time"]).timestamp()
#     concept_ids = question_kc_df[question_kc_df["question_id"] == qid]["knowledgecomponent_id"].tolist()

#     # Map concept ids
#     mapped = [kc2idx[k] for k in concept_ids if k in kc2idx]
#     if mapped:
#         model.forward_single_step(
#             student_id=39,
#             q_idx=torch.tensor(qid),
#             o_idx=torch.tensor(oid),
#             u_idx=torch.tensor(uid),
#             score=torch.tensor(score),
#             timestamp=torch.tensor(timestamp),
#             concept_ids=mapped
#         )

# # 3. Lấy trạng thái kiến thức hiện tại
# snapshot = model.get_snapshot(39)

# # 4. Các concept học sinh 39 từng gặp
# learned_concepts = set()
# for qid in history_df["question_id"]:
#     concept_ids = question_kc_df[question_kc_df["question_id"] == qid]["knowledgecomponent_id"].tolist()
#     learned_concepts.update(kc2idx[k] for k in concept_ids if k in kc2idx)

# # 5. Hiển thị kết quả
# print(f"\n Học sinh 39 đã học các khái niệm sau:")
# print("-" * 70)
# print(f"{'Concept Index':<15} | {'Tên khái niệm':<40} | {'‖Mv‖'}")
# print("-" * 70)
# for cid in sorted(learned_concepts):
#     try:
#         concept_id_real = kc_list[cid]
#         name = kc_df[kc_df["id"] == concept_id_real]["name"].values[0]
#         mv_norm = snapshot[cid].norm().item()
#         print(f"{cid:<15} | {name:<40} | {mv_norm:.4f}")
#     except Exception as e:
#         print(f"{cid:<15} | [ lỗi]: {e}")

# import matplotlib.pyplot as plt
# import networkx as nx

# # 1. Tạo đồ thị kiến thức tổng từ snapshot và graph của mô hình
# G_full = visualize_student_knowledge_graph(
#     student_id=39,
#     step=-1,
#     model=model,
#     show_prerequisite=False
# )

# # 2. Lọc đồ thị: chỉ giữ các node mà học sinh 39 đã từng học
# G_learned = G_full.subgraph(learned_concepts).copy()

# # 3. Hiển thị đồ thị đã lọc
# plt.figure(figsize=(12, 8))
# pos = nx.spring_layout(G_learned, seed=42)

# node_colors = [G_learned.nodes[n]["color"] for n in G_learned.nodes]
# node_labels = {n: G_learned.nodes[n]["name"] for n in G_learned.nodes}

# nx.draw_networkx_nodes(G_learned, pos, node_color=node_colors, node_size=700)
# nx.draw_networkx_edges(G_learned, pos, arrows=True)
# nx.draw_networkx_labels(G_learned, pos, labels=node_labels, font_size=10)

# plt.title("Bản đồ kiến thức của học sinh 39 (chỉ những khái niệm đã học)", fontsize=14)
# plt.axis("off")
# plt.tight_layout()
# plt.show()

# # 6. Dự đoán khả năng trả lời đúng với câu hỏi số 9
# target_qid = 9

# if target_qid not in question2idx:
#     raise ValueError("Câu hỏi 9 không tồn tại trong dữ liệu.")

# q_idx = torch.tensor(question2idx[target_qid])
# question_concepts = question_kc_df[question_kc_df["question_id"] == target_qid]["knowledgecomponent_id"].tolist()
# concept_ids = [kc2idx[k] for k in question_concepts if k in kc2idx]

# # Chọn option giả định (ví dụ option đầu tiên và unchosen là tiếp theo)
# option_ids = option_df[option_df["question_id"] == target_qid]["id"].tolist()
# if len(option_ids) < 2:
#     raise ValueError("Không đủ lựa chọn để tạo o_idx và u_idx cho câu hỏi 9.")
# o_idx = torch.tensor(option_ids[0])
# u_idx = torch.tensor(option_ids[1])
# score_dummy = torch.tensor(1)  # giả định trả lời đúng
# timestamp_now = torch.tensor(pd.Timestamp.now().timestamp())

# # Dự đoán
# with torch.no_grad():
#     pred = model.forward_single_step(
#         student_id=39,
#         q_idx=q_idx,
#         o_idx=o_idx,
#         u_idx=u_idx,
#         score=score_dummy,
#         timestamp=timestamp_now,
#         concept_ids=concept_ids
#     )

# print(f"\n[Dự đoán] Xác suất học sinh 39 trả lời đúng câu hỏi 9: {pred.item():.4f}")

"""#train

"""
# Configure device and optimize for RTX 5060 with CUDA 12.9
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Configure device and optimize for Intel i5-1135G7 CPU and MX350 GPU (CUDA 12.7)
if torch.cuda.is_available():
    # MX350 is a low-end GPU, so avoid enabling TF32 (not supported) and heavy autotuning
    torch.backends.cudnn.benchmark = False  # Disable cudnn auto-tuner (not beneficial for small/medium models)
    torch.backends.cuda.matmul.allow_tf32 = False  # MX350 does not support TF32
    torch.backends.cudnn.allow_tf32 = False

    # Set optimal thread count for CPU operations (i5-1135G7 has 4 cores/8 threads)
    torch.set_num_threads(8)

    # Print device info
    print(f"Using device: {torch.cuda.get_device_name()}")
    print(f"Available memory: {torch.cuda.get_device_properties(device).total_memory/1e9:.2f} GB")
else:
    # CPU only: set thread count for i5-1135G7 (4 cores/8 threads)
    torch.set_num_threads(8)
    print("Using CPU: Intel i5-1135G7 (8 threads)")
    print("Available RAM: 16 GB")

device

# model = model.to(device)

# torch.autograd.set_detect_anomaly(True)

code = """
import os
import torch
import pickle
import pandas as pd
import json
import networkx as nx
import matplotlib.pyplot as plt

def save_model_and_snapshot(model, fold, output_dir="outputs"):
    os.makedirs(output_dir, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(output_dir, f"best_model_fold{fold}.pth"))
    torch.save(model, os.path.join(output_dir, f"full_model_fold{fold}.pt"))
    with open(os.path.join(output_dir, f"snapshots_fold{fold}.pkl"), "wb") as f:
        pickle.dump(model.snapshots, f)
    with open(os.path.join(output_dir, f"memory_fold{fold}.pkl"), "wb") as f:
        pickle.dump(model.student_memory, f)
    print(f" Đã lưu model, snapshot và memory vào thư mục `{output_dir}/`")

def save_val_metrics(val_metrics, fold, output_dir="outputs"):
    os.makedirs(output_dir, exist_ok=True)
    path = os.path.join(output_dir, f"val_metrics_fold{fold}.json")
    with open(path, "w") as f:
        json.dump(val_metrics, f, indent=2)
    print(f" Đã lưu kết quả đánh giá vào `{path}`")

def export_mastery_csv(model, kc_list, concept_df, fold, output_dir="outputs/"):
    rows = []
    for student_id, memory_tensor in model.student_memory.items():
        for idx, vec in enumerate(memory_tensor):
            mastery_score = vec.norm().item()
            concept_id = kc_list[idx]
            concept_name = concept_df[concept_df["id"] == concept_id]["name"].values[0]
            level = (
                "Yếu" if mastery_score < 1.0 else
                "Trung bình" if mastery_score < 1.5 else
                "Tốt"
            )
            rows.append({
                "student_id": student_id,
                "concept_id": concept_id,
                "concept_name": concept_name,
                "mastery_score": round(mastery_score, 4),
                "mastery_level": level
            })
    df = pd.DataFrame(rows)
    os.makedirs(output_dir, exist_ok=True)
    path = os.path.join(output_dir, f"student_mastery_detail_fold{fold}.csv")
    df.to_csv(path, index=False)
    print(f" Đã lưu file: {path}")

def export_concept_links(model, concept_df, kc_list, fold, output_dir="outputs/"):
    rows = []
    for student_id, snapshots in model.snapshots_graph.items():
        if not snapshots:
            continue
        last_edge_index, last_edge_weight = snapshots[-1]
        for src, tgt, w in zip(last_edge_index[0], last_edge_index[1], last_edge_weight):
            src_id = int(src.item())
            tgt_id = int(tgt.item())
            src_name = concept_df[concept_df["id"] == src_id]["name"].values[0]
            tgt_name = concept_df[concept_df["id"] == tgt_id]["name"].values[0]
            rows.append({
                "student_id": student_id,
                "source_id": src_id,
                "source_name": src_name,
                "target_id": tgt_id,
                "target_name": tgt_name,
                "weight": round(w.item(), 4)
            })
    df = pd.DataFrame(rows)
    os.makedirs(output_dir, exist_ok=True)
    path = os.path.join(output_dir, f"student_concept_links_fold{fold}.csv")
    df.to_csv(path, index=False)
    print(f" Đã lưu file: {path}")

"""
with open("utils_dcrkt.py", "w", encoding="utf-8") as f:
    f.write(code)
print("✅ Đã tạo file utils_dcrkt.py thành công.")

import importlib
import utils_dcrkt as utils_dcrkt
importlib.reload(utils_dcrkt)

from utils_dcrkt import (
    save_model_and_snapshot,
    save_val_metrics,
    export_mastery_csv,
    export_concept_links

)

from collections import defaultdict
import pandas as pd
import torch
from torch.optim import Adam
from torch.nn import BCELoss
from tqdm import tqdm
from sklearn.metrics import roc_auc_score,accuracy_score, f1_score


def get_time_split_data(interaction_df, val_ratio=0.2):
    train_rows, val_rows = [], []
    for user_id, user_df in interaction_df.groupby("user_idx"):
        user_df = user_df.sort_values("timestamp")
        n = len(user_df)
        split_point = int(n * (1 - val_ratio))
        train_rows.append(user_df.iloc[:split_point])
        val_rows.append(user_df.iloc[split_point:])
    return pd.concat(train_rows).reset_index(drop=True), pd.concat(val_rows).reset_index(drop=True)


def train_dcrkt(args, interaction_df, fold, kc_list, concept_df):
    device = args.device

    # Loại học sinh có quá ít tương tác
    user_counts = interaction_df["user_idx"].value_counts()
    filtered_users = user_counts[user_counts >= 20].index
    interaction_df = interaction_df[interaction_df["user_idx"].isin(filtered_users)].reset_index(drop=True)

    # Tách train/val theo thời gian
    train_df, val_df = get_time_split_data(interaction_df, val_ratio=0.2)

    # Add unchosen column efficiently 
    for df in [interaction_df, train_df, val_df]:
        df.loc[:, "unchosen_idx"] = df["answer_idx"].fillna(0)

    # Khởi tạo mô hình
    model = DCRKT(
        num_c=args.concept, num_q=args.question, num_o=args.option,
        dim_q=args.q_dim, dim_g=args.g_dim, num_heads=args.heads,
        top_k=args.top_k, dropout=args.dropout,
        knowledge_mask=getattr(args, "knowledge_mask", None)
    ).to(device)

    optimizer = Adam(model.parameters(), lr=args.lr)
    loss_fn = BCELoss()
    best_val_loss = float("inf")
    patience = 0

    def group_by_user(df):
        # Pre-process data in batch
        df = df.copy()
        df["concept_idxs"] = df["concept_idxs"].apply(lambda x: eval(x) if isinstance(x, str) else x)
        
        user_data = defaultdict(list)
        for row in df.itertuples(index=False):
            try:
                user_data[row.user_idx].append({
                    "q_idx": int(row.question_idx),
                    "o_idx": int(row.answer_idx) if pd.notna(row.answer_idx) else 0,
                    "u_idx": int(row.unchosen_idx) if pd.notna(row.unchosen_idx) else int(row.answer_idx),
                    "score": float(row.is_correct),
                    "timestamp": float(row.timestamp),
                    "concept_ids": row.concept_idxs
                })
            except Exception as e:
                print(f"[ERROR] Bỏ qua row lỗi: {row}")
                print(e)
                continue
        return user_data

    train_user_data = group_by_user(train_df)

    # Pre-allocate tensors for batch processing
    for epoch in range(args.epoch):
        model.train()
        total_loss = 0.0
        total_samples = 0

        for student_id, interactions in tqdm(train_user_data.items(), desc=f"[Fold {fold}] Epoch {epoch+1}"):
            model.reset_memory(student_id)
            interactions = sorted(interactions, key=lambda x: x["timestamp"])
            
            # Process interactions in mini-batches
            batch_size = 32
            for i in range(0, len(interactions), batch_size):
                batch = interactions[i:i+batch_size]
                
                # Prepare batch tensors
                q_idxs = torch.tensor([s["q_idx"] for s in batch], dtype=torch.long).to(device)
                o_idxs = torch.tensor([s["o_idx"] for s in batch], dtype=torch.long).to(device)
                u_idxs = torch.tensor([s["u_idx"] for s in batch], dtype=torch.long).to(device)
                scores = torch.tensor([s["score"] for s in batch], dtype=torch.float).to(device)
                timestamps = torch.tensor([s["timestamp"] for s in batch], dtype=torch.float).to(device)
                concept_ids = [s["concept_ids"] for s in batch]
                
                # Process batch
                preds = []
                for j in range(len(batch)):
                    pred = model.forward_single_step(
                        student_id=student_id,
                        q_idx=q_idxs[j], o_idx=o_idxs[j], u_idx=u_idxs[j],
                        score=scores[j], timestamp=timestamps[j],
                        concept_ids=torch.tensor(concept_ids[j], dtype=torch.long).to(device)
                    )
                    preds.append(pred)
                
                preds = torch.stack(preds)
                loss = loss_fn(preds, scores)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item() * len(batch)
                total_samples += len(batch)

        avg_loss = total_loss / total_samples if total_samples > 0 else 0

        # Đánh giá và lưu
        val_metrics = evaluate_dcrkt(model, val_df, device)
        val_loss = val_metrics["val_loss"]

        print(f"[Epoch {epoch+1}] Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f} | AUC: {val_metrics['AUC']:.4f} | F1: {val_metrics['F1']:.4f}")

        if val_loss < best_val_loss:
            save_model_and_snapshot(model, fold)
            save_val_metrics(val_metrics, fold)
            export_mastery_csv(model, kc_list, concept_df, fold)
            # export_concept_links(model, concept_df, kc_list, fold)
            best_val_loss = val_loss
            patience = 0
            print(f" Mô hình improved tại epoch {epoch+1}")
        else:
            patience += 1
            if patience >= args.patience:
                print(f" Early stopping tại epoch {epoch+1}")
                break

    return best_val_loss, val_metrics



def test_dcrkt(model, test_df, device):
    model.eval()
    model.to(device)
    loss_fn = BCELoss()

    from collections import defaultdict
    student_data = defaultdict(list)

    for row in test_df.itertuples(index=False):
        try:
            student_data[row.user_idx].append({
                "q_idx": int(row.question_idx),
                "o_idx": int(row.answer_idx),
                "u_idx": int(getattr(row, "unchosen_idx", row.answer_idx)),
                "score": float(row.is_correct),
                "timestamp": float(row.timestamp),
                "concept_ids": eval(row.concept_ids) if isinstance(row.concept_ids, str) else row.concept_ids
            })
        except Exception as e:
            print(f"[ERROR] Bỏ qua row lỗi: {row}")
            print(e)

    y_true_all, y_pred_all = [], []
    total_loss, total_samples = 0.0, 0

    with torch.no_grad():
        for student_id, interactions in student_data.items():
            model.reset_memory(student_id)
            interactions = sorted(interactions, key=lambda x: x["timestamp"])

            for sample in interactions:
                q_idx = torch.tensor(sample["q_idx"], dtype=torch.long).to(device)
                o_idx = torch.tensor(sample["o_idx"], dtype=torch.long).to(device)
                u_idx = torch.tensor(sample["u_idx"], dtype=torch.long).to(device)
                score = torch.tensor(sample["score"], dtype=torch.float).to(device)
                timestamp = torch.tensor(sample["timestamp"], dtype=torch.float).to(device)
                concept_ids = torch.tensor(sample["concept_ids"], dtype=torch.long).to(device)

                pred = model.forward_single_step(
                    student_id=student_id,
                    q_idx=q_idx,
                    o_idx=o_idx,
                    u_idx=u_idx,
                    score=score,
                    timestamp=timestamp,
                    concept_ids=concept_ids
                )

                y_true_all.append(score.item())
                y_pred_all.append(pred.item())
                total_loss += loss_fn(pred, score).item()
                total_samples += 1

    return evaluate_metrics(y_true_all, y_pred_all, total_loss, total_samples)

def evaluate_dcrkt(model, val_df, device):
    model.eval()
    model.to(device)
    loss_fn = BCELoss()

    from collections import defaultdict
    student_data = defaultdict(list)

    for row in val_df.itertuples(index=False):
        try:
            student_data[row.user_idx].append({
                "q_idx": int(row.question_idx),
                "o_idx": int(row.answer_idx),
                "u_idx": int(getattr(row, "unchosen_idx", row.answer_idx)),
                "score": float(row.is_correct),
                "timestamp": float(row.timestamp),
                "concept_ids": eval(row.concept_idxs) if isinstance(row.concept_idxs, str) else row.concept_idxs
            })
        except Exception as e:
            print(f"[EVAL ERROR] Bỏ qua row lỗi: {row}")
            print(e)

    y_true_all, y_pred_all = [], []
    total_loss, total_samples = 0.0, 0

    with torch.no_grad():
        for student_id, interactions in student_data.items():
            model.reset_memory(student_id)
            interactions = sorted(interactions, key=lambda x: x["timestamp"])

            for sample in interactions:
                try:
                    q_idx = torch.tensor(sample["q_idx"], dtype=torch.long).to(device)
                    o_idx = torch.tensor(sample["o_idx"], dtype=torch.long).to(device)
                    u_idx = torch.tensor(sample["u_idx"], dtype=torch.long).to(device)
                    score = torch.tensor(sample["score"], dtype=torch.float).to(device)
                    timestamp = torch.tensor(sample["timestamp"], dtype=torch.float).to(device)
                    concept_ids = torch.tensor(sample["concept_ids"], dtype=torch.long).to(device)

                    pred = model.forward_single_step(
                        student_id=student_id,
                        q_idx=q_idx,
                        o_idx=o_idx,
                        u_idx=u_idx,
                        score=score,
                        timestamp=timestamp,
                        concept_ids=concept_ids
                    )

                    y_true_all.append(score.item())
                    y_pred_all.append(pred.item())
                    total_loss += loss_fn(pred, score).item()
                    total_samples += 1
                except Exception as e:
                    print(f"[EVAL ERROR] Sample lỗi: {e}")
                    continue

    return evaluate_metrics(y_true_all, y_pred_all, total_loss, total_samples)


def evaluate_metrics(y_true, y_pred, total_loss, total_samples):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    y_bin = (y_pred >= 0.5).astype(int)

    auc = roc_auc_score(y_true, y_pred) if len(set(y_true)) > 1 else 0.0
    acc = accuracy_score(y_true, y_bin)
    f1 = f1_score(y_true, y_bin)
    avg_loss = total_loss / total_samples if total_samples > 0 else float("inf")

    return {
        "AUC": round(float(auc), 4),
        "ACC": round(float(acc), 4),
        "F1": round(float(f1), 4),
        "val_loss": round(float(avg_loss), 4),
    }

import torch
import numpy as np
import pandas as pd
from itertools import chain
from argparse import Namespace

# ======= Gọi pipeline tiền xử lý =======
snapshots, concept_graph, concept_memory, interaction_df, num_q, num_c, num_o, relation_dict, knowledge_mask = run_all_steps()
# ======= Khởi tạo args =======
args = Namespace(
    device= device,
    k_fold=2,
    batch=32,
    epoch=1,
    lr=1e-3,
    patience=5,
    q_dim=64,
    g_dim=64,
    heads=4,
    top_k=5,
    dropout=0.2,
    question=None,
    option=None,
    user=None,
    concept=None
)

# ======= Cập nhật args từ interaction_df =======
args.question = int(interaction_df["question_idx"].max()) + 1
args.option   = int(interaction_df["answer_idx"].max()) + 1
args.user     = int(interaction_df["user_idx"].max()) + 1

if isinstance(interaction_df["concept_idxs"].iloc[0], str):
    interaction_df["concept_idxs"] = interaction_df["concept_idxs"].apply(eval)
args.concept = int(max(chain.from_iterable(interaction_df["concept_idxs"].tolist()))) + 1

# ======= TRAINING & ĐÁNH GIÁ QUA CÁC FOLD =======
val_results = []
val_losses = []

for fold in range(args.k_fold):
    print(f"\nTraining Fold {fold + 1}/{args.k_fold}")
    val_loss, val_metrics = train_dcrkt(args, interaction_df, fold, kc_list, concept_df)
    val_metrics["Fold"] = fold + 1
    val_results.append(val_metrics)
    val_losses.append(val_loss)

# ======= In thống kê kết quả sau các fold =======
df = pd.DataFrame(val_results)
print("\n Kết quả trung bình các fold:")
print(df.drop(columns=["Fold"]).mean().round(4))

print("\n Độ lệch chuẩn các fold:")
print(df.drop(columns=["Fold"]).std().round(4))
print(df.round(4))

